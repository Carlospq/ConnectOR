{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConectOR v2 (Conect Orthologue RNAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "import argparse\n",
    "import os, sys\n",
    "import json\n",
    "import wget\n",
    "import gzip\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import networkx as nx\n",
    "from os import path, listdir\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "### Dictionaris\n",
    "with open('dictionaries.json') as f:\n",
    "    dictionaries = json.load(f)\n",
    "    \n",
    "def check_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "\n",
    "#Only for jupyter\n",
    "from IPython.display import IFrame\n",
    "minMatch = 30\n",
    "gene_level = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not needed for jupyter\n",
    "#Options from command line\n",
    "parser = argparse.ArgumentParser(description = \"ConnectOR (v.1.0)\"\\\n",
    "                                               \"By: Carlos Pulido (carlos.pulido@dbmr.unibe.ch)\")\n",
    "\n",
    "\n",
    "def range_limited_int_type(arg):\n",
    "    \"\"\" Type function for argparse - an int within some predefined bounds \"\"\"\n",
    "    try:\n",
    "        i = int(arg)\n",
    "    except ValueError:    \n",
    "        raise argparse.ArgumentTypeError(\"Must be an integer\")\n",
    "    if i < 1 or i > 99:\n",
    "        raise argparse.ArgumentTypeError(\"Argument must be < \" + str(1) + \" and > \" + str(99) + \". Default: 30\")\n",
    "    return i\n",
    "\n",
    "\n",
    "parser.add_argument('-mM', '--minMatch',\n",
    "                    required = False,\n",
    "                    default = '30',\n",
    "                    action = 'store',\n",
    "                    dest = 'minMatch',\n",
    "                    type = range_limited_int_type,\n",
    "                    help = '0.N Minimum ratio of bases that must remap in liftOver step.'\\\n",
    "                           'Default: 30 (0.30 minimum ratio)')\n",
    "\n",
    "parser.add_argument('-g', '--gene',\n",
    "                    dest = 'gene_level',\n",
    "                    action = 'store_true',\n",
    "                    default = False,\n",
    "                    help = 'Generate results at gene level along with exon level results (default: False)')\n",
    "\n",
    "#Storing user options\n",
    "options = parser.parse_args()\n",
    "gene_level = options.gene_level\n",
    "minMatch = options.minMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "def assign_simplified_geneType(Type):\n",
    "    type_dict = {\"none\": [\"none\"],\n",
    "                 \"pc\": [\"protein_coding\", \"pc\"],\n",
    "                 \"ncRNA\": [\"NOVEL\", \"stringtie\", \"3prime_overlapping_ncRNA\", \"antisense\", \"bidirectional_promoter_lncRNA\", \"lincRNA\", \"ncRNA\",\n",
    "                           \"non_coding\", \"processed_transcript\", \"sense_intronic\", \"sense_overlapping\", \"lncRNA\", \"snoRNA\", \"snRNA\", \"sRNA\", \n",
    "                           \"misc_RNA\", \"rRNA\", \"scaRNA\", \"miRNA\", \"TEC\", \"scRNA\", \"macro_lncRNA\",\n",
    "                           \"pseudogene\", \"transcribed_processed_pseudogene\", \"translated_processed_pseudogene\", \"polymorphic_pseudogene\", \"processed_pseudogene\", \"rRNA_pseudogene\",\n",
    "                           \"transcribed_unitary_pseudogene\", \"transcribed_unprocessed_pseudogene\", \"unitary_pseudogene\", \"unprocessed_pseudogene\", \"translated_unprocessed_pseudogene\"],\n",
    "                 \"other\": [\"other\", \"IG_V_pseudogene\", \"IG_C_pseudogene\", \"IG_V_gene\", \"IG_C_gene\", \"IG_J_gene\", \"TR_J_pseudogene\", \"IG_J_pseudogene\", \"IG_pseudogene\", \"IG_D_pseudogene\", \n",
    "                           \"ribozyme\", \"vault_RNA\", \"vaultRNA\", \"TR_C_gene\", \"TR_J_gene\", \"TR_V_gene\", \"TR_V_pseudogene\", \"TR_D_gene\", \"IG_D_gene\", \"Mt_tRNA\", \"Mt_rRNA\", \"IG_LV_gene\"]}\n",
    "    biotype = [t for t in type_dict if Type in type_dict[t]]\n",
    "    if biotype == []: biotype = [\"ncRNA\"]\n",
    "    return(biotype[0])\n",
    "\n",
    "\n",
    "def read_config(file_name):\n",
    "    config_df =  pd.read_csv(file_name, sep='\\t', na_filter= False)\n",
    "    return(config_df)\n",
    "\n",
    "\n",
    "def check_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        \n",
    "\n",
    "def check_file(file_name_list):\n",
    "    for file_name in file_name_list:\n",
    "        if not os.path.isfile(file_name):\n",
    "            print(\"'%s' doesn not exist. Please check config_file. Exiting...\"%(file_name))\n",
    "            sys.exit(1)\n",
    "            \n",
    "\n",
    "def change_default(feature, value, config_df, i):\n",
    "    default_values = config_df.loc[i][\"default\"].split(\"|\")\n",
    "    if feature == \"gtf\": \n",
    "        n = 0\n",
    "    else:\n",
    "        n = 1\n",
    "    default_values[n] = value\n",
    "    return(\"|\".join(default_values))\n",
    "\n",
    "\n",
    "def download_default_files(file_url, folder_name):\n",
    "    check_folder(folder_name)\n",
    "    file_name = file_url.split(\"/\")[-1]\n",
    "    if not os.path.isfile(\"/\".join([folder_name, file_name])):\n",
    "        wget.download(file_url, folder_name)\n",
    "        print('\\t%s downloaded succesfully..'%(file_name))\n",
    "    else:\n",
    "        print('\\t%s already exists.. Skipping'%(file_name))\n",
    "\n",
    "\n",
    "def arguments_dic(line):\n",
    "    args = line[-1].split(\";\")\n",
    "    dic = {}\n",
    "    for e in args:\n",
    "        if e == \";\": continue\n",
    "        if e == \"\": continue\n",
    "        if e[0] == \" \": e = e[1:]\n",
    "        key = e.split(\" \")[0].replace(\"\\\"\",\"\")\n",
    "        dic[key] = e.split(\" \")[1].replace(\"\\\"\",\"\")\n",
    "    return dic\n",
    "\n",
    "\n",
    "def biotype(keys, arguments):\n",
    "    biotype=''\n",
    "    while not biotype:\n",
    "        for k in keys:\n",
    "            try:\n",
    "                biotype = arguments[k]\n",
    "                return(biotype)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if not biotype: biotype = \"NOVEL\"\n",
    "        return(biotype)\n",
    "\n",
    "\n",
    "#def generate_maps(gtf, sp):\n",
    "def generate_maps(arg_list):\n",
    "\n",
    "    gtf = arg_list[0]\n",
    "    sp = arg_list[1]\n",
    "    print(\"\\tMaps for %s... generating\\n\"%(sp))\n",
    "    \n",
    "    if gtf.endswith(\".gz\"):\n",
    "        f = gzip.open(gtf, 'rb')\n",
    "        compressed = True\n",
    "    else:\n",
    "        f = open(gtf, 'r')\n",
    "        compressed = False\n",
    "        \n",
    "    genes = {}\n",
    "\n",
    "    for line in f:\n",
    "        if compressed: line = str(line, 'utf-8')\n",
    "        if line.startswith(\"#\"): continue\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        arguments = arguments_dic(line)\n",
    "\n",
    "        if \"gene_name\" in arguments:\n",
    "            gene_name = arguments[\"gene_name\"]\n",
    "        else:\n",
    "            gene_name = arguments[\"gene_id\"]\n",
    "\n",
    "        geneID = arguments[\"gene_id\"]\n",
    "        gene_biotype = biotype([\"gene_type\", \"gene_biotype\"], arguments)\n",
    "        gene_biotype = assign_simplified_geneType(gene_biotype)\n",
    "        if line[2] == \"exon\":\n",
    "            if not geneID in genes:\n",
    "                genes[geneID] = {\"gene_name\": gene_name,\n",
    "                                 \"gene_type\": gene_biotype,\n",
    "                                 \"coordinates\": [line[0],line[3],line[4],line[6]],\n",
    "                                 \"transcripts\": []}\n",
    "                \n",
    "            transcriptID = arguments[\"transcript_id\"]\n",
    "            if not transcriptID in genes[geneID][\"transcripts\"]:\n",
    "                genes[geneID][\"transcripts\"].append(transcriptID)\n",
    "            \n",
    "            exon_start = line[3]\n",
    "            exon_end = line[4]\n",
    "            if int(exon_start) < int(genes[geneID][\"coordinates\"][1]): genes[geneID][\"coordinates\"][1] = exon_start\n",
    "            if int(exon_end) > int(genes[geneID][\"coordinates\"][2]): genes[geneID][\"coordinates\"][2] = exon_end\n",
    "            \n",
    "            #Update gene biotype; protein_coding prevail other biotypes\n",
    "            if genes[geneID][\"gene_type\"] == gene_biotype or genes[geneID][\"gene_type\"] == \"protein_coding\":\n",
    "                continue\n",
    "            else:\n",
    "                genes[geneID][\"gene_type\"] = gene_biotype\n",
    "\n",
    "\n",
    "            \n",
    "    fo = open(\"maps/\"+sp+\".geneID_map.txt\", 'w')\n",
    "    for g in genes:\n",
    "        line = \"\\t\".join([g, genes[g][\"gene_name\"], genes[g][\"gene_type\"], \":\".join(genes[g][\"coordinates\"]), \"|\".join(genes[g][\"transcripts\"])])\n",
    "        fo.write(line+\"\\n\")\n",
    "    fo.close()\n",
    "    print(\"\\tMaps for %s... generated \"%(sp))\n",
    "    \n",
    "    \n",
    "#def generate_beds(gtf, sp, gene_level):\n",
    "def generate_beds(arg_list):\n",
    "    \n",
    "    gtf = arg_list[0]\n",
    "    sp = arg_list[1]\n",
    "    gene_level = arg_list[2]\n",
    "    if gtf.endswith(\".gz\"):\n",
    "        f = gzip.open(gtf, 'rb')\n",
    "        compressed = True\n",
    "    else:\n",
    "        f = open(gtf, 'r')\n",
    "        compressed = False\n",
    "\n",
    "    print(\"\\tBEDs for %s... generating\\n\"%(sp))\n",
    "    genes = {}\n",
    "    output_exons = open(\"./BEDs/{}.exons.bed\".format(sp),\"w\")\n",
    "    for line in f:\n",
    "        if compressed: line = str(line, 'utf-8')\n",
    "        if line.startswith(\"#\"): continue\n",
    "        line = line.strip().split(\"\\t\")\n",
    "\n",
    "        if line[2] != \"exon\": continue\n",
    "        arguments = arguments_dic(line)\n",
    "\n",
    "        if \"gene_name\" in arguments:\n",
    "            gene_name = arguments[\"gene_name\"]\n",
    "        else:\n",
    "            gene_name = arguments[\"gene_id\"]\n",
    "            \n",
    "        gene_id = arguments[\"gene_id\"]\n",
    "        chrom = line[0] if line[0].startswith(\"chr\") else \"chr\"+line[0]\n",
    "        start = str(int(line[3])-1)\n",
    "        end = line[4]\n",
    "        strand = line[6]\n",
    "        \n",
    "        coordiantes = \":\".join([chrom, start, end, strand])\n",
    "        exon_id = \"|\".join([gene_id, gene_name, coordiantes])\n",
    "        \n",
    "        if int(start) < 0: start = \"0\"\n",
    "        if not gene_id in genes:\n",
    "            genes[gene_id] = {\"chrom\": chrom, \n",
    "                              \"start\": int(start),\n",
    "                              \"end\": int(end),\n",
    "                              \"strand\": strand,\n",
    "                              \"gene_name\": gene_name}\n",
    "        else:\n",
    "            if int(start) < genes[gene_id][\"start\"]:\n",
    "                genes[gene_id][\"start\"] = int(start)\n",
    "            if int(end) > genes[gene_id][\"end\"]:\n",
    "                genes[gene_id][\"end\"] = int(end)\n",
    "\n",
    "        exon_bed_line = \"\\t\".join([chrom, start, end, exon_id, '0', strand])+\"\\n\"\n",
    "        output_exons.write(exon_bed_line)\n",
    "    output_exons.close()\n",
    "    \n",
    "    if gene_level:\n",
    "        output_genes = open(\"./BEDs/{}.genes.bed\".format(sp),\"w\")\n",
    "        for gene in genes:\n",
    "            d = genes[gene]\n",
    "            chrom = d[\"chrom\"]\n",
    "            start = str(d[\"start\"])\n",
    "            end = str(d[\"end\"])\n",
    "            strand = d[\"strand\"]\n",
    "            gene_name = d[\"gene_name\"]\n",
    "\n",
    "            coordiantes = \":\".join([chrom, start, end, strand])\n",
    "            gene_id = \"|\".join([gene, gene_name, coordiantes])\n",
    "        \n",
    "            gene_bed_line = \"\\t\".join([chrom, start, end, gene_id, '0', strand])+\"\\n\"\n",
    "            output_genes.write(gene_bed_line)\n",
    "        output_genes.close()\n",
    "    print(\"\\r\\tBEDs for %s... generated \"%(sp))\n",
    "    \n",
    "    \n",
    "def bed_sort():\n",
    "    files = dirs = os.listdir(\"./BEDs/\")\n",
    "    for file_name in files:\n",
    "        print(\"\\r\\t\"+file_name+\"... sorting\", end=\"\")\n",
    "        call = \"sort -u -k1,1 -k2,2n -o '%s' '%s'\"%(\"./BEDs/\"+file_name,\"./BEDs/\"+file_name)    \n",
    "        subprocess.call(call, shell=True)\n",
    "        print(\"\\r\\t\"+file_name+\"... sorted \")\n",
    "        \n",
    "\n",
    "def liftOver(arg_list):\n",
    "    sp1 = arg_list[0]\n",
    "    sp2 = arg_list[1]\n",
    "    feature = arg_list[2]\n",
    "    mM = arg_list[3]\n",
    "    \n",
    "    print(\"\\t{} {} to {}... mapping\\n\".format(sp1, feature, sp2))\n",
    "    oldFile = \"BEDs/{}.{}.bed\".format(sp1, feature)\n",
    "    map_chain = [chain for chain in config_df.loc[config_df[\"assembly_version\"]==sp1, \"chainmap\"].iloc[0].split(\",\") if sp2 in chain.lower()][0]\n",
    "    newFile = \"liftovers/{}to{}.{}.liftover\".format(sp1, sp2, feature)\n",
    "    unMapped= \"liftovers/{}to{}.{}.unmapped\".format(sp1, sp2, feature)\n",
    "    os.system(\"./scripts/liftOver -minMatch={} {} {} {} {}\".format(mM/100, oldFile, map_chain, newFile, unMapped))\n",
    "    print(\"\\t{} {} to {}... done         \".format(sp1, feature, sp2))\n",
    "    \n",
    "    \n",
    "def geneID_map_to_dict(file_name, dl = \"\\t\"):\n",
    "    dict_ = {}\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(dl)\n",
    "            dict_[line[0]] = {\"gene_name\": line[1],\n",
    "                              \"gene_type\": line[2],\n",
    "                              \"coordinates\": line[3],\n",
    "                              \"transcript_IDs\": line[4].split(\"|\")}                              \n",
    "    return(dict_)\n",
    "\n",
    "\n",
    "def parse_orthologs(line):\n",
    "    geneA_ID = line[0]\n",
    "    geneA_biotype = line[1]\n",
    "    geneB_IDs = line[2].split(\",\")\n",
    "    nexon = line[3].split(\",\")\n",
    "    pcent = line[4].split(\",\")\n",
    "    geneB_biotypes = line[5].split(\",\")\n",
    "\n",
    "    return(geneA_ID, geneA_biotype, geneB_IDs, nexon, pcent, geneB_biotypes)\n",
    "\n",
    "\n",
    "def count_biotypes(geneB_biotypes):\n",
    "    tmp   = []\n",
    "    none  = [\"none\"]\n",
    "    pc    = [\"protein_coding\", \"pc\"]\n",
    "    ncRNA = [\"NOVEL\", \"ncRNA\", \"stringtie\", \"3prime_overlapping_ncRNA\", \"antisense\", \"bidirectional_promoter_lncRNA\", \"lincRNA\",\n",
    "             \"non_coding\", \"processed_transcript\", \"sense_intronic\", \"sense_overlapping\", \"lncRNA\", \"snoRNA\", \"snRNA\", \"sRNA\", \n",
    "             \"misc_RNA\", \"rRNA\", \"scaRNA\", \"miRNA\", \"TEC\", \"scRNA\", \"macro_lncRNA\",\n",
    "             \"pseudogene\", \"transcribed_processed_pseudogene\", \"translated_processed_pseudogene\", \"polymorphic_pseudogene\", \"processed_pseudogene\", \"rRNA_pseudogene\",\n",
    "             \"transcribed_unitary_pseudogene\", \"transcribed_unprocessed_pseudogene\", \"unitary_pseudogene\", \"unprocessed_pseudogene\", \"translated_unprocessed_pseudogene\"]\n",
    "    other = [\"IG_V_pseudogene\", \"IG_C_pseudogene\", \"IG_V_gene\", \"IG_C_gene\", \"IG_J_gene\", \"TR_J_pseudogene\", \"IG_J_pseudogene\", \"IG_pseudogene\", \"IG_D_pseudogene\", \n",
    "             \"ribozyme\", \"vault_RNA\", \"TR_C_gene\", \"TR_J_gene\", \"TR_V_gene\", \"TR_V_pseudogene\", \"TR_D_gene\", \"IG_D_gene\", \"Mt_tRNA\", \"Mt_rRNA\", \"IG_LV_gene\"]\n",
    "    \n",
    "    for type_ in geneB_biotypes:\n",
    "        if type_ in none:\n",
    "            tmp.append(\"none\")\n",
    "        elif type_ in ncRNA:\n",
    "            tmp.append(\"lncRNA\")\n",
    "        elif type_ in pc:\n",
    "            tmp.append(\"pc\")\n",
    "        else:\n",
    "            tmp.append(\"other\")\n",
    "    \n",
    "    biotype_counts = [tmp.count(\"none\"), tmp.count(\"lncRNA\"), tmp.count(\"pc\"), tmp.count(\"other\")]\n",
    "    classification = classify_biotypes(biotype_counts)\n",
    "    \n",
    "    return(biotype_counts, classification)\n",
    "\n",
    "\n",
    "def classify_biotypes(bcounts):\n",
    "    classification = \"other\"\n",
    "    #unique cases\n",
    "    #  none              lncRNA            pc                other\n",
    "    if bcounts[0]==1 and bcounts[1]==0 and bcounts[2]==0 and bcounts[3]==0:\n",
    "        classification = \"none\"\n",
    "    if bcounts[0]>=0 and bcounts[1]==1 and bcounts[2]==0 and bcounts[3]==0:\n",
    "        classification = \"lncRNA\"\n",
    "    if bcounts[0]>=0 and bcounts[1]==0 and bcounts[2]==1 and bcounts[3]==0:\n",
    "        classification = \"pc\"\n",
    "    if bcounts[0]>=0 and bcounts[1]==0 and bcounts[2]==0 and bcounts[3]==1:\n",
    "        classification = \"other\"\n",
    "\n",
    "    #multiple cases\n",
    "    #  none              lncRNA            pc                other\n",
    "    if bcounts[0]>1  and bcounts[1]==0 and bcounts[2]==0 and bcounts[3]==0:\n",
    "        classification = \"nones\"\n",
    "    if bcounts[0]>=0 and bcounts[1]>1  and bcounts[2]==0 and bcounts[3]==0:\n",
    "        classification = \"lncRNAs\"\n",
    "    if bcounts[0]>=0 and bcounts[1]==0 and bcounts[2]>1  and bcounts[3]==0:\n",
    "        classification = \"pcs\"\n",
    "    if bcounts[0]>=0 and bcounts[1]==0 and bcounts[2]==0 and bcounts[3]>1:\n",
    "        classification = \"others\"\n",
    "\n",
    "    #dual cases\n",
    "    #  none              lncRNA            pc                other\n",
    "    if bcounts[0]>=0 and bcounts[1]>=1 and bcounts[2]>=1 and bcounts[3]>=0:\n",
    "        classification = \"lncRNA_PC\"\n",
    "    if bcounts[0]>=0 and bcounts[1]>=1 and bcounts[2]==0 and bcounts[3]>=1:\n",
    "        classification = \"lncRNA_other\"\n",
    "    if bcounts[0]>=0 and bcounts[1]==0 and bcounts[2]>=1 and bcounts[3]>=1:\n",
    "        classification = \"PC_other\"\n",
    "\n",
    "    return(classification)\n",
    "\n",
    "\n",
    "def generate_components_stats(components, G):\n",
    "    x_to_comp_idx = {}\n",
    "    comp_idx_x = {}\n",
    "    for idx,c in enumerate(components):\n",
    "\n",
    "        s = G.subgraph(c)\n",
    "        edges = s.edges()    \n",
    "        edges_set = set(edges)\n",
    "        nodes = []\n",
    "        links = []\n",
    "        for gene in c:\n",
    "            #gene[1]=GeneName; idx=cluster_id; gene=tuple(sp, GeneName, GeneBiotype) \n",
    "            x_to_comp_idx[gene[1]] = {\"idx\": idx, \"node\": gene}\n",
    "\n",
    "        for edge in edges:\n",
    "\n",
    "            #G.has_edge(node1, node2)\n",
    "            sps_in_edge = [node[0] for node in edge]\n",
    "            is_bidirectional = 1 if (edge[1],edge[0]) in edges_set else 0\n",
    "            l1 = list(edge)\n",
    "            #1=edge is bidirectional; 2=edge is unidirectional\n",
    "            l1.append(is_bidirectional if is_bidirectional else 2)\n",
    "            links.append(tuple(l1))\n",
    "\n",
    "            n1 = list(edge[0])\n",
    "            n2 = list(edge[1])\n",
    "            #1=gene has bidirectional prediction; 2=gene has unidirectional prediction; 3=gene has no prediction\n",
    "            if is_bidirectional:\n",
    "                n1.append(1)\n",
    "                nodes.append(tuple(n1))\n",
    "            else:\n",
    "                n1.append(2)\n",
    "                n2.append(3)           \n",
    "                nodes.append(tuple(n1))\n",
    "                nodes.append(tuple(n2))\n",
    "\n",
    "        #bidirectionality = True: returns only nodes instead of edges and counts of ocurrences for each node\n",
    "        comp_idx_x[idx] = {\"bidirectional\": Counter(nodes),\n",
    "                           \"links\": links}\n",
    "    return(x_to_comp_idx, comp_idx_x)\n",
    "\n",
    "\n",
    "def plot_component(g, dictionary = {\"hg38\": \"lightgreen\", \"mm10\": \"lightblue\", \"danrer11\": \"yellow\"}):\n",
    "    org_color = dictionary\n",
    "    fig,ax = plt.subplots(figsize=(8,8))\n",
    "    nodes = g.nodes()\n",
    "    edges = g.edges()    \n",
    "    edges_set = set(edges)\n",
    "    bidirectional = [True if (v[1],v[0]) in edges_set else False for v in edges]\n",
    "    edge_color = ['red' if x else 'gray' for x in bidirectional]\n",
    "    node_color = [org_color[x[0]] for x in nodes]\n",
    "    labels = {x:x[1] for x in nodes} ############### cambiar x[1] por tu_dictionary[x[1]]!\n",
    "    pos=nx.spring_layout(g)    \n",
    "    nx.draw_networkx_nodes(g,pos,nodelist=nodes,node_color=node_color)\n",
    "    nx.draw_networkx_edges(g,pos,edgelist=edges,edge_color=edge_color)\n",
    "    nx.draw_networkx_labels(g,pos,labels,font_weight='bold')\n",
    "    plt.legend(handles=[mpatches.Patch(color=v, label=k) for k,v in org_color.items()],\n",
    "               bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_clusters_stats(components, G, level, output_path=\"./counts/\", species_names=False):\n",
    "    if species_names:\n",
    "        species = species_names\n",
    "    else:\n",
    "        species = sps.keys()\n",
    "        \n",
    "    #Biotypes accepted for ncRNA clusters\n",
    "    ncrna_bt = [\"NOVEL\", \"stringtie\", \"3prime_overlapping_ncRNA\", \"antisense\", \"bidirectional_promoter_lncRNA\", \"lincRNA\", \"ncRNA\",\n",
    "                \"non_coding\", \"processed_transcript\", \"sense_intronic\", \"sense_overlapping\", \"lncRNA\", \"snoRNA\", \"snRNA\", \"sRNA\", \n",
    "                \"misc_RNA\", \"rRNA\", \"scaRNA\", \"miRNA\", \"TEC\", \"scRNA\", \"macro_lncRNA\",\n",
    "                \"pseudogene\", \"transcribed_processed_pseudogene\", \"translated_processed_pseudogene\", \"polymorphic_pseudogene\", \"processed_pseudogene\", \"rRNA_pseudogene\",\n",
    "                \"transcribed_unitary_pseudogene\", \"transcribed_unprocessed_pseudogene\", \"unitary_pseudogene\", \"unprocessed_pseudogene\", \"translated_unprocessed_pseudogene\"]\n",
    "    \n",
    "    #Check input/output names\n",
    "    if not path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    if species_names:\n",
    "        o_name = \"\".join([species_names[0],\"_\", species_names[1], \"_\"])\n",
    "    else: \n",
    "        o_name = \"\"\n",
    "        \n",
    "    output_cluster_stats = path.join(output_path, '%scluster_stats_%s.csv'%(o_name, level))\n",
    "    output_genes_stats   = path.join(output_path, '%sgenes_stats_%s.csv'%(o_name, level))    \n",
    "    \n",
    "    org_all_count = list(species)\n",
    "    \n",
    "    #Total genes\n",
    "    total_genes = len(G.nodes())\n",
    "    \n",
    "    #Map with full name of genes\n",
    "    map_fileNames = [f for f in os.listdir(\"maps\")]\n",
    "    map_dictionary = {}\n",
    "    for fileName in map_fileNames:\n",
    "        df_maps = pd.read_csv(\"maps/\"+fileName, sep='\\t', na_filter= False, header=None, names=[\"geneID\", \"geneName\", \"biotype\", \"coordinates\", \"transcripts\"])\n",
    "        df_maps[\"fullID\"] = df_maps[\"geneID\"]+\";\"+df_maps[\"geneName\"]+\";\"+df_maps[\"biotype\"]+\";\"+df_maps[\"coordinates\"]\n",
    "        df_maps = df_maps[[\"geneID\", \"fullID\"]].set_index('geneID').to_dict()[\"fullID\"]\n",
    "        map_dictionary.update(df_maps)\n",
    "    \n",
    "    #Liftover genes dictionary\n",
    "    liftovers = {level: {}}\n",
    "    liftovers_set = set()\n",
    "    liftover_fileNames = [f for f in os.listdir(\"liftovers\") if \"liftover\" in f and level in f]\n",
    "    for fileName in liftover_fileNames:\n",
    "      \n",
    "        name = fileName.split(\".\")\n",
    "        sp1 = name[0].split(\"to\")[0]\n",
    "        sp2 = name[0].split(\"to\")[1]\n",
    "        \n",
    "        if not sp1 in liftovers[level]:\n",
    "            liftovers[level][sp1] = {}\n",
    "\n",
    "        df_liftovers = pd.read_csv(\"liftovers/\"+fileName, sep='\\t', na_filter= False, header=None, names=[\"chrom\", \"start\", \"end\", \"id\", \"score\", \"strand\"])  \n",
    "        df_liftovers['id'] = df_liftovers['id'].apply(lambda x:[y for y in x.split(\"|\")])\n",
    "        df_set = set(df_liftovers['id'].apply(lambda x: x[0]))\n",
    "        \n",
    "        liftovers_set.update(df_set)\n",
    "        liftovers[level][sp1][sp2] = df_set\n",
    "\n",
    "\n",
    "    #Headers for output files\n",
    "    header_stats = ['Cluster ID','Nodes','Number of species', 'Bidirectionality', 'Cluster type', 'Biotypes']\n",
    "    header_genes = ['Gene ID','Species', 'Biotype', 'Cluster ID', 'Cluster Biotype', 'Cluster type', 'Orthologues', 'in_degree','out_degree']\n",
    "    \n",
    "    for sp in species:\n",
    "        header_stats += ['has_'+sp, 'count_'+sp, 'Gene IDs '+sp]\n",
    "        header_genes.insert(6, 'Gene to '+sp)\n",
    "        \n",
    "    o1 = open(output_cluster_stats,'w')\n",
    "    o1.write(','.join(header_stats)+'\\n')\n",
    "    o2 = open(output_genes_stats,'w')\n",
    "    o2.write(','.join(header_genes)+'\\n')\n",
    "    \n",
    "    #Iterate over clusters\n",
    "    for idx,c in enumerate(components):\n",
    "        g = G.subgraph(c)\n",
    "        nodes = g.nodes()\n",
    "        edges = g.edges()\n",
    "        edges_set = set(edges)\n",
    "        print(\"\\r%.2f%s Cluster %s of %s (%s genes in cluster)\"%(int(idx)/len(components)*100, \"%\", str(idx), str(len(components)), str(len(g.nodes()))) , end= \"\\r\", file=sys.stderr)\n",
    "\n",
    "        #Bidirectionality in the cluster\n",
    "        is_bidirectional = [1 if (v[1],v[0]) in edges_set else 0 for v in edges]\n",
    "        bidirectional = 0 if len(edges) == 0 else sum(is_bidirectional)/len(edges)\n",
    "        \n",
    "        #Biotypes in cluster\n",
    "        biotypes = set([n[2] for n in nodes])\n",
    "        if len(biotypes) == 1:\n",
    "            cluster_biotype = biotypes.pop()\n",
    "        else:\n",
    "            cluster_biotype = \"other\"\n",
    "\n",
    "        #Nodes per species in cluster\n",
    "        org_comp_count_ugly = Counter([x[0] for x in nodes])\n",
    "        org_comp_count = {k:v for k,v in org_comp_count_ugly.items()}\n",
    "        for sp in species:\n",
    "            if sp not in org_comp_count:\n",
    "                org_comp_count[sp] = 0\n",
    "        n_species = sum([1 for sp in org_comp_count if org_comp_count[sp] > 0])\n",
    "        \n",
    "        #Cluster data\n",
    "        data = {'Cluster ID': str(idx),\n",
    "                'Nodes': str(len(nodes)),\n",
    "                'Number of species': str(n_species),\n",
    "                'Bidirectionality': '%.2f'%(bidirectional),\n",
    "                'Biotypes': cluster_biotype}\n",
    "        \n",
    "        for sp in species:\n",
    "            data['has_'+sp] = str(int(org_comp_count[sp]>0))\n",
    "            data['count_'+sp] = str(org_comp_count[sp])\n",
    "            data['per_cluster_'+sp] = '%.2f'%(100*org_comp_count[sp]/len(nodes))\n",
    "            data['per_total_'+sp] = '%.2f'%(100*org_comp_count[sp]/total_genes)\n",
    "            data['Gene IDs '+sp] = \"|\".join([map_dictionary[n[1]] for n in nodes if n[0]==sp])\n",
    "            \n",
    "        #Cluster type\n",
    "        cluster_type=''\n",
    "        has_list = [int(data[x]) for x in data if x.startswith(\"has\")]      \n",
    "        count_list = [int(data[x]) for x in data if x.startswith(\"count\")]\n",
    "                \n",
    "        if len(nodes) == 1:\n",
    "            cluster_type = 'One to none' if list(nodes)[0][1] in liftovers_set else \"Not lifted\"\n",
    "        \n",
    "        if len(nodes) == sum(has_list) and len(nodes) > 1:\n",
    "            cluster_type = 'One to one' if bidirectional == 1 else \"One to half\"\n",
    "        \n",
    "        if len(nodes) > sum(has_list):\n",
    "            cluster_type = 'One to many' if 1 in count_list else \"Many to many\"\n",
    "        \n",
    "        data['Cluster type'] = cluster_type\n",
    "        o1.write(','.join(data[x] for x in header_stats)+'\\n')\n",
    "        \n",
    "        \n",
    "        #Write info for each node (gene) to output file 2\n",
    "        for node in nodes:\n",
    "            \n",
    "            in_edges  = set(g.in_edges(node))\n",
    "            out_edges = set(g.out_edges(node))\n",
    "            \n",
    "            suc = set(g.successors(node))\n",
    "            pre = set(g.predecessors(node))\n",
    "\n",
    "            union = suc.union(pre)\n",
    "            inter = suc.intersection(pre)\n",
    "                \n",
    "            predictions = \"|\".join([map_dictionary[n[1]] for n in union])\n",
    "            node_biotype = node[2]\n",
    "            data2 = {'Gene ID': node[1],\n",
    "                     'Species': node[0],\n",
    "                     'Biotype': node_biotype,\n",
    "                     'Cluster ID': str(idx),\n",
    "                     'Cluster Biotype': cluster_biotype,\n",
    "                     'Cluster type': data['Cluster type'],\n",
    "                     'Orthologues': predictions,\n",
    "                     'Gene to '+node[0]: \"\",\n",
    "                     'in_degree': str(g.in_degree(node)),\n",
    "                     'out_degree': str(g.out_degree(node))}\n",
    "            \n",
    "            #Get connections in cluster only for sp1 and sp2\n",
    "            for sp1 in species:\n",
    "                if not sp1 == node[0]: continue\n",
    "                for sp2 in species:\n",
    "                    if sp1 == sp2: continue\n",
    "                    if sp2 == node[0]:\n",
    "                        data2[\"Gene to \"+sp2] = \"\"\n",
    "                        continue\n",
    "                    if sp2 in [n[0] for n in suc]:\n",
    "                        data2[\"Gene to \"+sp2] = \"Predicted\"\n",
    "                    elif node[1] in liftovers[level][sp1][sp2]:\n",
    "                        data2[\"Gene to \"+sp2] = \"Lifted\"\n",
    "                    else:\n",
    "                        data2[\"Gene to \"+sp2] = \"Not lifted\"\n",
    "\n",
    "            o2.write(','.join(data2[x] for x in header_genes)+'\\n')\n",
    "    \n",
    "    o1.close()\n",
    "    print('Wrote',output_cluster_stats, ' '*20, file=sys.stderr)\n",
    "    o2.close()\n",
    "    print('Wrote',output_genes_stats, ' '*20, file=sys.stderr)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read and check config file\n",
    "print(\"Checking Config_file...\")\n",
    "\n",
    "config_df = read_config(\"./config\")\n",
    "config_df.set_index(\"species\", inplace = True)\n",
    "config_df[\"default\"] = \"False|False\"\n",
    "for i in config_df.index:\n",
    "    \n",
    "    #Check if assembly_version exists in dictionaries\n",
    "    if not config_df.loc[i][\"assembly_version\"].lower() in dictionaries[\"chain_maps\"]:\n",
    "        print(\"'%s' is not a valid assembly_version. Please use one of the following values for default analysis: %s\"%(config_df.loc[i][\"assembly_version\"], \",\".join(dictionaries[\"chain_maps\"])))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    #Add and download default annotations in config if not provided\n",
    "    if not config_df.loc[i][\"annotation\"]:\n",
    "        default_gtf = dictionaries[\"gtfs_ensembl_r98\"][config_df.loc[i][\"assembly_version\"].lower()].split(\"/\")[-1]\n",
    "        config_df.at[i, 'default'] = change_default(\"gtf\", \"True\", config_df, i)\n",
    "        config_df.at[i, 'annotation'] = \"GTFs/\"+default_gtf\n",
    "        download_default_files(dictionaries[\"gtfs_ensembl_r98\"][config_df.loc[i][\"assembly_version\"]], \"GTFs\")\n",
    "    else:\n",
    "        check_file([config_df.loc[i][\"annotation\"]])\n",
    "        \n",
    "    #Add and download default chainmaps in config if not provided\n",
    "    if not config_df.loc[i][\"chainmap\"]:\n",
    "        chainmaps = []\n",
    "        config_df.at[i, 'default'] = change_default(\"chainmap\", \"True\", config_df, i)\n",
    "        for j in config_df.index:\n",
    "            if i!=j: \n",
    "                default_chainmap_path = dictionaries[\"chain_maps\"][config_df.loc[i][\"assembly_version\"].lower()][config_df.loc[j][\"assembly_version\"].lower()]\n",
    "                default_chainmap_name = \"chainmaps/\"+default_chainmap_path.split(\"/\")[-1]\n",
    "                chainmaps.append(default_chainmap_name)\n",
    "                download_default_files(default_chainmap_path, \"chainmaps\")\n",
    "        config_df.at[i, 'chainmap'] = \",\".join(chainmaps)\n",
    "    else:\n",
    "        check_file(config_df.loc[i][\"chainmap\"].split(\",\"))\n",
    "                \n",
    "print(\"Config_file is correct..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GeneID - geneName - geneBiotype - transcripts maps\n",
    "check_folder(\"maps\")\n",
    "print(\"Generating geneID - geneName - geneBiotype - transcripts maps...\")\n",
    "\n",
    "gtfFiles_assembly = list(zip(list(config_df[\"annotation\"]), list(config_df[\"assembly_version\"])))\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "pool.map(generate_maps, gtfFiles_assembly)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_folder(\"maps\")\n",
    "# print(\"Generating transcriptID-geneID & geneID-geneName-geneType maps...\")\n",
    "# for i in config_df.index:\n",
    "#     gtf_file = config_df.loc[i][\"annotation\"]\n",
    "#     sp_v = config_df.loc[i][\"assembly_version\"].lower()\n",
    "#     print(\"\\r\\tMaps for %s... generating\"%(sp_v), end=\"\")\n",
    "#     generate_maps(gtf_file, sp_v)\n",
    "#     print(\"\\r\\tMaps for %s... generated \"%(sp_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BEDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate BED with genes/exons from GTF files\n",
    "check_folder(\"BEDs\")\n",
    "print(\"Generating BED files for exons and genes...\")\n",
    "\n",
    "gtfFiles_assembly_level = list( zip( list(config_df[\"annotation\"]), list(config_df[\"assembly_version\"]), [gene_level]*len(list(config_df[\"assembly_version\"])) ) )\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "pool.map(generate_beds, gtfFiles_assembly_level)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in config_df.index:\n",
    "#     gtf_file = config_df.loc[i][\"annotation\"]\n",
    "#     sp_v = config_df.loc[i][\"assembly_version\"].lower()\n",
    "#     if config_df.loc[i][\"default\"].split(\"|\")[0] == \"True\":\n",
    "#         gtf_file = \"GTFs/\"+gtf_file\n",
    "#     print(\"\\tBEDs for %s... generating\"%(sp_v), end=\"\")\n",
    "#     generate_beds(gtf_file, sp_v, gene_level)\n",
    "#     print(\"\\r\\tBEDs for %s... generated \"%(sp_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort BED files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sorting BED files...\")\n",
    "bed_sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LifOver exons/genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LifOver exons/genes\n",
    "features = [\"exons\", \"genes\"] if gene_level else [\"exons\"] \n",
    "arguments = []\n",
    "for sp1 in config_df[\"assembly_version\"]:\n",
    "    for sp2 in config_df[\"assembly_version\"]:\n",
    "        if sp1 == sp2: continue\n",
    "        for feature in features:\n",
    "            arg_list = [sp1, sp2, feature, minMatch]\n",
    "            arguments.append(arg_list)\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "pool.map(liftOver, arguments)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [\"exons\", \"genes\"]\n",
    "# check_folder(\"liftovers\")\n",
    "# print(\"LiftOver...\")\n",
    "# for i in config_df.index:\n",
    "#     sp_vi = config_df.loc[i][\"assembly_version\"].lower()\n",
    "#     chainmaps = config_df.loc[i][\"chainmap\"].split(\",\")\n",
    "#     n=0\n",
    "#     for j in config_df.index:\n",
    "#         if i == j: continue\n",
    "#         sp_vj = config_df.loc[j][\"assembly_version\"].lower()\n",
    "#         map_chain = chainmaps[n] if config_df.loc[i][\"default\"] else chainmaps[n]\n",
    "#         n+=1\n",
    "#         #liftOver oldFile map.chain newFile unMapped\n",
    "#         for feature in features:\n",
    "#             if feature == \"genes\" and not gene_level: continue\n",
    "#             print(\"\\r\\t{} {} to {}... mapping\".format(i, feature, j))\n",
    "#             oldFile = \"BEDs/{}.{}.bed\".format(sp_vi, feature)\n",
    "#             newFile = \"liftovers/{}to{}.{}.liftover\".format(sp_vi, sp_vj, feature)\n",
    "#             unMapped= \"liftovers/{}to{}.{}.unmapped\".format(sp_vi, sp_vj, feature)\n",
    "#             os.system(\"./scripts/liftOver -minMatch={} {} {} {} {}\".format(minMatch/100, oldFile, map_chain, newFile, unMapped))\n",
    "#             print(\"\\r\\t{} {} to {}... done         \".format(i, feature, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersect LiftOvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intersect LiftOvers\n",
    "check_folder(\"overlaps\")\n",
    "print(\"Intersecting LiftOver...\")\n",
    "for i in config_df.index:\n",
    "    for j in config_df.index:\n",
    "        if i == j: continue\n",
    "        for f in [\"exons\", \"genes\"]:\n",
    "            if f == \"genes\" and not gene_level: continue\n",
    "            print(\"\\r\\t{} {} to {}... intersecting\".format(i, f, j), end=\"\")\n",
    "            sp1=config_df.loc[i][\"assembly_version\"].lower()\n",
    "            sp2=config_df.loc[j][\"assembly_version\"].lower()\n",
    "            lifover_input = 'liftovers/%sto%s.%s.liftover'%(sp1, sp2, f)\n",
    "            bed_input = 'BEDs/%s.%s.bed'%(sp2, f)\n",
    "            output = 'overlaps/%sto%s.%s.overlap'%(sp1, sp2, f)\n",
    "            call = 'intersectBed -wao -s -a %s -b %s > %s'%(lifover_input, bed_input, output)\n",
    "            subprocess.call(call, shell=True, executable='/bin/bash')\n",
    "            print(\"\\r\\t{} {} to {}... done         \".format(i, f, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse Overlap\n",
    "check_folder(\"orthology\")\n",
    "print(\"Predicting orthologues...\")\n",
    "\n",
    "for i in config_df.index:\n",
    "    for j in config_df.index:\n",
    "        \n",
    "        if i == j: continue\n",
    "        \n",
    "        sp1=config_df.loc[i][\"assembly_version\"].lower()\n",
    "        sp2=config_df.loc[j][\"assembly_version\"].lower() \n",
    "\n",
    "        sp1_gID_Name_Type = geneID_map_to_dict('maps/%s.geneID_map.txt'%(sp1))\n",
    "        sp2_gID_Name_Type = geneID_map_to_dict('maps/%s.geneID_map.txt'%(sp2))\n",
    "        \n",
    "        for f in [\"exons\", \"genes\"]:\n",
    "            if f == \"genes\" and not gene_level: continue\n",
    "            print(\"\\r\\t{} {} to {}... finding orthologues\".format(i, f, j), end=\"\")\n",
    "            overlaps = \"overlaps/%sto%s.%s.overlap\"%(sp1, sp2, f)\n",
    "            genes = {}\n",
    "            for line in open(overlaps, 'r'):\n",
    "                # read line overlap between sp1 and sp2\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                exonA = line[3]\n",
    "                geneA_ID = exonA.split(\"|\")[0]\n",
    "                exonB = line[9]\n",
    "                geneB_ID = exonB.split(\"|\")[0]\n",
    "                \n",
    "                # calculate overlapping %\n",
    "                lA = int(line[2])-int(line[1]) #length geneA\n",
    "                lB = int(line[8])-int(line[7]) #length geneB\n",
    "                o = int(line[12])              #overlap\n",
    "\n",
    "                rA = (o*100)/lA                    #overlap of A to B\n",
    "                rB = 0 if lB == 0 else (o*100)/lB  #overlap of B to A\n",
    "\n",
    "                # Harmonic mean\n",
    "                try:\n",
    "                    hm = 2.0/((1.0/rA)+(1.0/rB))\n",
    "                except ZeroDivisionError:\n",
    "                    hm = 0\n",
    "                    \n",
    "                # Gene_Name and Gene_Type\n",
    "                geneAtype = \"none\" if geneA_ID == \".\" else sp1_gID_Name_Type[geneA_ID][\"gene_type\"]                \n",
    "                geneBtype = \"none\" if geneB_ID == \".\" else sp2_gID_Name_Type[geneB_ID][\"gene_type\"]\n",
    "                \n",
    "                # Add sp1 gene overlaps to dictionary\n",
    "                if not geneA_ID in genes:\n",
    "                    genes[geneA_ID] = {\"geneAtype\": geneAtype,\n",
    "                                       \"overlaps\": [],\n",
    "                                       \"geneBtypes\": [],\n",
    "                                       \"exons\": {}}\n",
    "\n",
    "                if not geneB_ID in genes[geneA_ID][\"overlaps\"]:\n",
    "                    genes[geneA_ID][\"overlaps\"].append(geneB_ID)\n",
    "                    genes[geneA_ID][\"geneBtypes\"].append(geneBtype)\n",
    "                    \n",
    "                # Add sp1 exon overlaps to dictionary\n",
    "                if not exonA in genes[geneA_ID][\"exons\"]:\n",
    "                    genes[geneA_ID][\"exons\"][exonA] = {}\n",
    "                \n",
    "                if not geneB_ID in genes[geneA_ID][\"exons\"][exonA]:\n",
    "                    genes[geneA_ID][\"exons\"][exonA][geneB_ID] = [rA, rB, hm]  \n",
    "                \n",
    "                # Update exonB if new exon have highr A>B overlap\n",
    "                # NOTE ---> and higher B>A overlap? change % of overlap with HM ??\n",
    "                if rA > genes[geneA_ID][\"exons\"][exonA][geneB_ID][0]:\n",
    "                    genes[geneA_ID][\"exons\"][exonA][geneB_ID] = [rA, rB, hm]\n",
    "                    \n",
    "            # Summarize overlaps into output\n",
    "            output_file = open('orthology/%sto%s.%s'%(sp1, sp2, f), \"w\")\n",
    "            header = \"\\t\".join([\"GeneA_ID\", \"GeneA_biotype\", \"GeneB_IDs\", \"GeneB_biotypes\", \"GeneB_Classification\", \"GeneA_number_overlapping_exons\", \"GeneA_percent_overlapping\"])\n",
    "            output_file.write(header+\"\\n\")\n",
    "            for geneA_ID in genes:\n",
    "                overlaps = genes[geneA_ID][\"overlaps\"]\n",
    "                geneAtype = genes[geneA_ID][\"geneAtype\"]\n",
    "                geneBtype = genes[geneA_ID][\"geneBtypes\"]\n",
    "                number_of_exons = []\n",
    "                percentageA = []\n",
    "                percentageB = []\n",
    "                for geneB_ID in overlaps:\n",
    "                    n = 0\n",
    "                    mA, mB = 0, 0\n",
    "                    for exon in genes[geneA_ID][\"exons\"]:\n",
    "                        if geneB_ID in genes[geneA_ID][\"exons\"][exon]:\n",
    "                            n+=1\n",
    "                            # %overlap A>B (rA)\n",
    "                            mA+=genes[geneA_ID][\"exons\"][exon][geneB_ID][0]\n",
    "                            mB+=genes[geneA_ID][\"exons\"][exon][geneB_ID][1]\n",
    "                    number_of_exons.append(str(n))\n",
    "                    percentageA.append(str(mA/n))\n",
    "                    percentageB.append(str(mB/n))\n",
    "\n",
    "                percentageA = [ float(x) for x in percentageA ]\n",
    "                percentageB = [ float(x) for x in percentageB ]\n",
    "                number_of_exons = [ int(x) for x in number_of_exons ]\n",
    "                \n",
    "                percentageA, number_of_exons, overlaps, geneBtype = zip(*sorted(zip(percentageA, number_of_exons, overlaps, geneBtype), reverse=True))\n",
    "                if \".\" in overlaps and len(overlaps)>1:\n",
    "                    overlaps = list(overlaps)\n",
    "                    geneBtype = list(geneBtype)\n",
    "                    overlaps.remove('.')\n",
    "                    geneBtype.remove(\"none\")\n",
    "                    overlaps = tuple(overlaps)\n",
    "                    geneBtype = tuple(geneBtype)\n",
    "                percentageA = [ str(x) for x in percentageA ]\n",
    "                number_of_exons = [ str(x) for x in number_of_exons ]\n",
    "                biotype_counts, classification = count_biotypes(geneBtype)\n",
    "                new_line = \"\\t\".join([geneA_ID, geneAtype, \",\".join(overlaps), \",\".join(geneBtype), classification, \",\".join(number_of_exons), \",\".join(percentageA)])\n",
    "                output_file.write(new_line+\"\\n\")\n",
    "            output_file.close()\n",
    "            \n",
    "            print(\"\\r\\t{} {} to {}... done               \".format(i, f, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read orthology into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESULTS\n",
    "input_path = './orthology'\n",
    "files = [x for x in listdir(input_path)]\n",
    "\n",
    "sps = {}\n",
    "for input_file in files:\n",
    "    #Names of sps being analyzed\n",
    "    org_source = input_file.split('.')[0].split('to')[0]\n",
    "    org_destin = input_file.split('.')[0].split('to')[1]\n",
    "    \n",
    "    #Read orthology results from spA to spB\n",
    "    df = pd.read_csv(path.join(input_path, input_file),\n",
    "                     sep='\\t',\n",
    "                     skiprows=1,\n",
    "                     names=[\"GeneA_ID\", \"GeneA_biotype\", \"GeneB_IDs\", \"GeneB_biotypes\", \"GeneB_Classification\", \"GeneA_number_overlapping_exons\", \"GeneA_percent_overlapping\"])\n",
    "    gene_info = df.set_index('GeneA_ID').T.to_dict()\n",
    "    \n",
    "    #Count total genes in spA and total genes lifted to spB\n",
    "    genes_org_source = len(open(\"maps/\"+org_source+\".geneID_map.txt\", 'r').readlines())    \n",
    "    exons_lifted = set([x.strip().split(\"\\t\")[3] for x in  open(\"liftovers/\"+org_source+\"to\"+org_destin+\".exons.liftover\", 'r').readlines()])\n",
    "    if gene_level:\n",
    "        genes_lifted = set([x.strip().split(\"\\t\")[3] for x in  open(\"liftovers/\"+org_source+\"to\"+org_destin+\".genes.liftover\", 'r').readlines()])\n",
    "    else:\n",
    "        genes_lifted = set()\n",
    "    \n",
    "    #Initialize gene_info dictionary and add total number of genes in spA\n",
    "    if not org_source in sps:\n",
    "        sps[org_source] = {\"total_genes\": genes_org_source}\n",
    "        \n",
    "    #Add information of lifted genes to spB\n",
    "    if not org_destin in sps[org_source]:\n",
    "        if \"exon\" in input_file:\n",
    "            feature = \"exon\"\n",
    "        else:\n",
    "            feature = \"gene\"\n",
    "        sps[org_source][org_destin] = {\"total_genes\": genes_org_source,\n",
    "                                       \"total_exons_lifted\": exons_lifted,\n",
    "                                       \"total_genes_lifted\": genes_lifted,\n",
    "                                       \"one_to_one\": 0,\n",
    "                                       \"one_to_many\": 0,\n",
    "                                       \"many_to_many\": 0,\n",
    "                                       \"gene_info\": {\"exon\": \"\",\n",
    "                                                     \"gene\": \"\"}}\n",
    "        sps[org_source][org_destin][\"gene_info\"][feature] = gene_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Clustered genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeneMaps into dictionary by species\n",
    "gene_type = {}\n",
    "for input_file in [x for x in os.listdir(\"./maps\")]:\n",
    "    sp = input_file.split(\".\")[0]\n",
    "    gene_type[sp] = geneID_map_to_dict('maps/%s.geneID_map.txt'%(sp))\n",
    "    \n",
    "# Orthology input files    \n",
    "input_path = './orthology'\n",
    "orthology_files = [x for x in listdir(input_path)]\n",
    "\n",
    "#Generate clusters\n",
    "ALL_GENES = []\n",
    "EDGES = {\"exon\": [],\n",
    "         \"gene\": []}\n",
    "\n",
    "for file in orthology_files:\n",
    "\n",
    "    feature = \"exon\" if \"exons\" in file else \"gene\"\n",
    "    \n",
    "    #Species being analyzed\n",
    "    org_source = file.split('.')[0].split('to')[0]\n",
    "    org_destin = file.split('.')[0].split('to')[1]\n",
    "\n",
    "    #Add all genes to list    \n",
    "    df = pd.read_csv(\"./maps/%s.geneID_map.txt\"%(org_source), sep=\"\\t\", header=None,\n",
    "                     names = [\"GeneA_ID\", \"GeneA_name\", \"Gene_biotype\", \"coordinates\", \"transcripts\"])\n",
    "    ALL_GENES += [(org_source, x, gene_type[org_source][x][\"gene_type\"]) for x in list(df['GeneA_ID'].unique())]\n",
    "    ALL_GENES = sorted(set(ALL_GENES))\n",
    "    \n",
    "    #Read pd's with classification\n",
    "    df = pd.read_csv(\"./orthology/%s\"%(file), sep=\"\\t\", header=1,\n",
    "                     names = [\"GeneA_ID\", \"GeneA_biotype\", \"GeneB_IDs\", \"GeneB_biotypes\", \"GeneB_Classification\", \"GeneA_number_overlapping_exons\", \"GeneA_percent_overlapping\"])\n",
    "    df = df[['GeneA_ID','GeneA_biotype', 'GeneB_IDs']]\n",
    "\n",
    "    \n",
    "\n",
    "    #Prepare df's\n",
    "    df['GeneB_IDs'] = df['GeneB_IDs'].apply(lambda x:[y for y in x.split(',') if y != '.'])\n",
    "\n",
    "    #Create edges (gene connections/predictions)\n",
    "    for row in df.itertuples():\n",
    "        geneA_ID = row.GeneA_ID\n",
    "        geneA_biotype = row.GeneA_biotype\n",
    "\n",
    "        for geneB_ID in row.GeneB_IDs:\n",
    "            geneB_biotype = gene_type[org_destin][geneB_ID][\"gene_type\"]\n",
    "            edge = ((org_source, geneA_ID, geneA_biotype), (org_destin, geneB_ID, geneB_biotype)) \n",
    "            EDGES[feature].append(edge)\n",
    "            \n",
    "# Create network (clusters)\n",
    "G_exon = nx.DiGraph()\n",
    "G_exon.add_nodes_from(ALL_GENES)\n",
    "G_exon.add_edges_from(EDGES[\"exon\"])\n",
    "G_exon_undirected = G_exon.to_undirected()\n",
    "components_exon = sorted(list(G_exon_undirected.subgraph(c) for c in nx.connected_components(G_exon_undirected)),key=lambda x:-len(x))\n",
    "print('EXON graph: %d nodes, %d edges, %d connected_components'%(nx.number_of_nodes(G_exon),\n",
    "    nx.number_of_edges(G_exon),len(components_exon)),file=sys.stderr)\n",
    "\n",
    "if gene_level:\n",
    "    G_gene = nx.DiGraph()\n",
    "    G_gene.add_nodes_from(ALL_GENES)\n",
    "    G_gene.add_edges_from(EDGES[\"gene\"])\n",
    "    G_gene_undirected = G_gene.to_undirected()\n",
    "    components_gene = sorted(list(G_gene_undirected.subgraph(c) for c in nx.connected_components(G_gene_undirected)),key=lambda x:-len(x))\n",
    "    print('GENE graph: %d nodes, %d edges, %d connected_components'%(nx.number_of_nodes(G_gene),\n",
    "        nx.number_of_edges(G_gene),len(components_gene)),file=sys.stderr)\n",
    "\n",
    "exon_to_comp_idx, comp_idx_exon = generate_components_stats(components_exon, G_exon)\n",
    "if gene_level:\n",
    "    gene_to_comp_idx, comp_idx_gene = generate_components_stats(components_gene, G_gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all species\n",
    "get_clusters_stats(components_exon, G_exon, \"exon\")\n",
    "if gene_level:\n",
    "    get_clusters_stats(components_gene, G_gene, \"gene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 1 vs 1 species\n",
    "species = list(sps.keys())\n",
    "if len(species) > 2:\n",
    "    for sp1_idx in range(0,len(species)):\n",
    "        for sp2_idx in range(0,len(species)):\n",
    "            if sp1_idx <= sp2_idx: continue\n",
    "            sp1 = species[sp1_idx]\n",
    "            sp2 = species[sp2_idx]\n",
    "\n",
    "            G_exon = nx.DiGraph()\n",
    "            G_exon.add_nodes_from([node for node in ALL_GENES if node[0] in [sp1, sp2]])\n",
    "            G_exon.add_edges_from([edge for edge in EDGES[\"exon\"] if edge[0][0] in [sp1, sp2] and  edge[1][0] in [sp1, sp2]])\n",
    "            G_exon_undirected = G_exon.to_undirected()\n",
    "            components_exon = sorted(list(G_exon_undirected.subgraph(c) for c in nx.connected_components(G_exon_undirected)),key=lambda x:-len(x))\n",
    "            exon_to_comp_idx, comp_idx_exon = generate_components_stats(components_exon, G_exon)\n",
    "            get_clusters_stats(components_exon, G_exon, \"exon\", species_names=[sp1, sp2])\n",
    "\n",
    "            if gene_level:\n",
    "                G_gene = nx.DiGraph()\n",
    "                G_gene.add_nodes_from([node for node in ALL_GENES if node[0] in [sp1, sp2]])\n",
    "                G_gene.add_edges_from([edge for edge in EDGES[\"gene\"] if edge[0][0] in [sp1, sp2] and  edge[1][0] in [sp1, sp2]])\n",
    "                G_gene_undirected = G_gene.to_undirected()\n",
    "                components_gene = sorted(list(G_gene_undirected.subgraph(c) for c in nx.connected_components(G_gene_undirected)),key=lambda x:-len(x))\n",
    "\n",
    "                gene_to_comp_idx, comp_idx_gene = generate_components_stats(components_gene, G_gene)\n",
    "                get_clusters_stats(components_gene, G_gene, \"gene\", species_names=[sp1, sp2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting results\n",
    "check_folder(\"plots\")\n",
    "print(\"\\rPlotting results...\", end=\"\")\n",
    "subprocess.call(\"Rscript ./scripts/ConnectOR_GenePlots.R \"+str(gene_level)+\" > /dev/null 2>/dev/null\", shell=True)\n",
    "print(\"\\rPlotting results... done\")\n",
    "\n",
    "# End\n",
    "print(\"\\nConnectOR finished correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only for jupyter\n",
    "IFrame(\"plots/gene_stats_all.pdf\", width=700, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only for jupyter\n",
    "IFrame(\"plots/clusters_stats_all.pdf\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only for jupyter\n",
    "## Plotting components in jupyter\n",
    "component_number = 506\n",
    "\n",
    "Ge = G_exon.subgraph(components_exon[component_number])\n",
    "plot_component(Ge)\n",
    "\n",
    "#Gg = G_gene.subgraph(components_exon[component_number])\n",
    "#plot_component(Gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
